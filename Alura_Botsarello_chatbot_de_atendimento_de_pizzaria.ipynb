{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnUUGqlYZ6QC8G3DvpSSug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcadecaro/Alura_Gemini_Botsarello/blob/main/Alura_Botsarello_chatbot_de_atendimento_de_pizzaria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Estudo de caso para o Gemini API**:\n",
        "\n",
        "Agentes e a chamada automática de funções com um chatbot de atendimento de pizzaria.\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1hwDVe9cNba5bZlAgBKATsEnoIW6dJpJ5?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Rodar no Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "U6McHXvLoV21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Botsarello** - Chatbot de atendimento de Pizzaria"
      ],
      "metadata": {
        "id": "kJsqLUHgk9me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agentes de IA** - O assunto do momento\n",
        "\n",
        "Os agentes de inteligência artificial (IA) estão se tornando cada vez mais importantes em diversos setores devido à sua capacidade de automatizar tarefas, tomar decisões complexas e interagir com o ambiente de forma autônoma.\n",
        "A execução eficaz de funções é um dos principais fatores que impulsionam a adoção dessa tecnologia, e espera-se que o impacto dos agentes de IA continue crescendo nos próximos anos."
      ],
      "metadata": {
        "id": "1jNQlt8ZlS8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Objetivos do projeto**\n",
        "\n",
        "\n",
        "\n",
        "1.   **Atendimento ao Cliente**: Respondem perguntas, resolvem problemas e fornecem suporte técnico de forma personalizada.\n",
        "2.   **Tomada de Decisões**: Agentes de IA podem receber algumas funções e tomar a decisão de utilizá-las conforme uma orientação apontada em prompts para atingir um objetivo.\n",
        "\n"
      ],
      "metadata": {
        "id": "0ecQ5jKAlUn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook mostra um exemplo prático de como usar a chamada automática de funções com o SDK Python da API do Gemini para construir um agente. Iremos definir algumas funções que compõem o sistema de pedidos de pizza, conectá-las à API do Gemini e criar um chat, que será implementado com um loop de interações do agente com o cliente da pizzaria.\n",
        "\n",
        "**O guia foi inspirado no prompt do Barista bot no estilo ReAct disponível através do AI Studio.**"
      ],
      "metadata": {
        "id": "SaE2Pv7GqFtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mãos na massa!**\n"
      ],
      "metadata": {
        "id": "5LE6anurpUXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalação da biblioteca: google-generativeai"
      ],
      "metadata": {
        "id": "w5O53t0Cse4x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3IzLYKxmTHd5"
      },
      "outputs": [],
      "source": [
        "!pip install -qU google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFjRBXVrAdYB"
      },
      "source": [
        "Para executar este notebook, sua **chave de API** deve estar armazenada na **área protegida do Colab** com o nome **`GOOGLE_API_KEY`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0gOuwcCUTNAO"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "from typing import Iterable\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.api_core import retry\n",
        "\n",
        "from google.colab import userdata\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desenvolvendo uma API simulando o Sistema de Pedidos da Pizzaria\n",
        "\n",
        "Para simular o sistema de pedidos da sua pizzaria, vamos definir algumas funções para gerenciar o pedido do cliente:.\n",
        "\n",
        "1. add_to_order - Adicionar\n",
        "2. remove_item - Editar\n",
        "3. clear_order - limpar\n",
        "4. confirm_order - Confirmar\n",
        "5. place_order - Finalizar\n",
        "\n",
        "Estas funções rastreiam o pedido do cliente usando as variáveis globais **order** (*o pedido em andamento*) e **placed_order** (*o pedido confirmado enviado para a cozinha*). Cada uma das funções de edição de pedido atualiza o **pedido** e, uma vez confirmado, o pedido é copiado para **pedido_confirmado** e logo em seguida resetado.\n"
      ],
      "metadata": {
        "id": "UeblO6r3vIxd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wMltPyUpTu3h"
      },
      "outputs": [],
      "source": [
        "order = []  # o pedido em andamento.\n",
        "placed_order = []  # o pedido confirmado enviado para a cozinha.\n",
        "\n",
        "def add_to_order(flavor: str, modifiers: Iterable[str] = ()) -> None:\n",
        "  \"\"\"Adds the specified flavor to the customer's order, including any modifiers.\"\"\"\n",
        "  order.append((flavor, modifiers))\n",
        "\n",
        "\n",
        "def get_order() -> Iterable[tuple[str, Iterable[str]]]:\n",
        "  \"\"\"Returns the customer's order.\"\"\"\n",
        "  return order\n",
        "\n",
        "\n",
        "def remove_item(n: int) -> str:\n",
        "  \"\"\"Remove the nth (one-based) item from the order.\n",
        "\n",
        "  Returns:\n",
        "    The item that was removed.\n",
        "  \"\"\"\n",
        "  item, modifiers = order.pop(int(n) - 1)\n",
        "  return item\n",
        "\n",
        "def send_invoice() -> None:\n",
        "  \"\"\"send an invoice to the customer.\"\"\"\n",
        "  # TODO(you!): Implement email.\n",
        "  print('  Nota fical enviada para customer@mail.com')\n",
        "\n",
        "\n",
        "def clear_order() -> None:\n",
        "  \"\"\"Removes all items from the customer's order.\"\"\"\n",
        "  order.clear()\n",
        "\n",
        "\n",
        "def confirm_order() -> str:\n",
        "  \"\"\"Asks the customer if the order is correct.\n",
        "\n",
        "  Returns:\n",
        "    The user's free-text response.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Seu Pedido:')\n",
        "  if not order:\n",
        "    print('  (Nenhum item encontrado)')\n",
        "\n",
        "  for flavor, modifiers in order:\n",
        "    print(f'  {flavor}')\n",
        "    if modifiers:\n",
        "      print(f'   - {\", \".join(modifiers)}')\n",
        "\n",
        "  return input('O pedido está correto? ')\n",
        "\n",
        "\n",
        "def place_order() -> int:\n",
        "  \"\"\"Submit the order to the kitchen.\n",
        "\n",
        "  Returns:\n",
        "    The estimated number of minutes until the order is ready.\n",
        "  \"\"\"\n",
        "  placed_order[:] = order.copy()\n",
        "  clear_order()\n",
        "  send_invoice()\n",
        "\n",
        "  # TODO(you!): Implement pizza fulfilment.\n",
        "  return randint(1, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7l3_vBtC0oF"
      },
      "source": [
        "## Teste de mesa da API\n",
        "\n",
        "Vamos testar algumas funções para nos certificarmos que o fluxo de nossa simulação está funcionando como deveria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jg1LjYNUWnsC",
        "outputId": "3fb8ca45-c2dc-4381-c701-be433629b1f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seu Pedido:\n",
            "  Mussarela\n",
            "   - borda de catupiry\n",
            "  Banana\n",
            "   - Borda de Nutela\n",
            "O pedido está correto? sim\n"
          ]
        }
      ],
      "source": [
        "# Teste de mesa!\n",
        "\n",
        "clear_order()\n",
        "add_to_order('Mussarela', ['borda de catupiry'])\n",
        "add_to_order('Jiló')\n",
        "remove_item(2)\n",
        "add_to_order('Banana', ['Borda de Nutela'])\n",
        "confirm_order();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOhiADoCC811"
      },
      "source": [
        "## Vamos especificar o comportamento do Botsarello no prompt\n",
        "\n",
        "Aqui vamos vamos definir o prompt do atendente de pizzaria. Nesse prompt vamos passar o cardapio da pizzaria e algumas opções para cada sabor.\n",
        "\n",
        "As instruções que passaremos devem incluir orientações e exemplos de como as funções devem ser chamadas. (exemplo. \"Sempre `confirm_order` com o cliente antes de `place_order`\"). Você pode passar o seu proprio estilo de interação para o bot, por exemplo se você quiser que o bot repita o que foi pedido pelo cliente depois de interpretar o pedido, passe instruções para isso.\n",
        "\n",
        "O final do prompt inclui alguns jargões que o bot pode encontrar, bem como instruções do dia - neste caso, ele observa que a pizzaria está com falta de Jiló.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IoBvZ1JYXgn5"
      },
      "outputs": [],
      "source": [
        "PIZZA_BOT_PROMPT = \"\"\"\\You are a pizza order taking system and you are restricted to talk only about pizzas on the MENU. Do not talk about anything but ordering MENU pizzas for the customer, ever.\n",
        "Your goal is to do place_order after understanding the menu items and any modifiers the customer wants.\n",
        "Add items to the customer's order with add_to_order, remove specific items with remove_item, and reset the order with clear_order.\n",
        "To see the contents of the order so far, call get_order (by default this is shown to you, not the user)\n",
        "Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will display the order items to the user and returns their response to seeing the list. Their response may contain modifications.\n",
        "Always verify and respond with drink and modifier names from the MENU before adding them to the order.\n",
        "If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect.\n",
        "You only have the modifiers listed on the menu below: Milk options, espresso shots, caffeine, sweeteners, special requests.\n",
        "Once the customer has finished ordering items, confirm_order and then place_order.\n",
        "\n",
        "\n",
        "Horário de Funcionamento: Terças aos Domingos das 5pm às 2am\n",
        "Preços: Todas as pizzas tem preço fixo de R$ 60,00.\n",
        "\n",
        "MENU:\n",
        "Bordas para todas as pizzas:\n",
        "borda de catupiry\n",
        "borda de cheddar\n",
        "borda de parmesão\n",
        "borda de nutela\n",
        "\n",
        "Pizzas Classicas:\n",
        "Mussarela\n",
        "Napolitana\n",
        "Calabresa\n",
        "Marguerita\n",
        "\n",
        "Pizzas Sobremesa:\n",
        "Banana\n",
        "Chocolate\n",
        "Morango com Chocolate\n",
        "\n",
        "Pizzas Stranger Things:\n",
        "Jiló\n",
        "\n",
        "\n",
        "Modifiers:\n",
        "borda de catupiry, borda de cheddar, borda de parmesão, borda de nutela\n",
        "Pedidos Customizados: any reasonable modification that does not involve items not on the menu, for example: 'Sem Queijo', 'Queijo Extra', 'Borda Dupla', etc.\n",
        "\n",
        "\n",
        "We have run out of Jiló today, so Jiló is not available.\n",
        "Respond always in Portugues do Brasil.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_ybYQ-sU7rn"
      },
      "source": [
        "## Configurando o modelo\n",
        "\n",
        "Nesta etapa, você reúne as funções em um Ordering_system(\"sistema\") que é passado como `tools`(ferramentas) e instancia o modelo para iniciar a sessão de bate-papo.\n",
        "\n",
        "Este bloco inclui duas opções para interagir com a API do Gemini. Ao alternar use_sys_inst, você pode alternar entre usar o Gemini 1.5 Pro com uma instrução de sistema (a mais alta qualidade, mas a cota gratuita pode ser insuficiente para uma longa sessão de bate-papo) ou o Gemini 1.0 Pro (cota gratuita mais alta, mas não suporta instruções de sistema).\n",
        "\n",
        "Uma função send_message retentável também é definida para ajudar em conversas com cota baixa.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9cXUOtEfX0fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurações de criatividade"
      ],
      "metadata": {
        "id": "-eDZQrUfWhjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos usar apenas algumas\n",
        "\n",
        "| Atributo          | Descrição                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
        "| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| `candidate_count` | (Opcional) Número inteiro (`int`) indicando a quantidade de respostas geradas a serem retornadas. Esse valor deve estar entre 1 e 8, inclusive. Se não definido, o padrão será 1.                                                                                                                             |\n",
        "| `temperature`     | (Opcional) Valor `float` que controla a aleatoriedade da saída. Valores entre 0.0 e 1.0 (inclusive) podem ser usados. Um valor mais próximo de 1.0 produzirá respostas mais variadas e criativas, enquanto um valor mais próximo de 0.0 normalmente resultará em respostas diretas do modelo. |"
      ],
      "metadata": {
        "id": "pldTFGW6Wloz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the model\n",
        "# TEMPERATURA / QUANTO MAIOR O MUMERO MAIS IA + CRIATIVA\n",
        "# TEMPERATURA / QUANTO MENOR O NUMERO MAIS IA + PROFISIONAL/FORMAL\n",
        "\n",
        "generation_config = {\n",
        "  \"temperature\": 0.5,\n",
        "  \"candidate_count\": 1,\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "8y8rdVQisdla"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurações de segurança"
      ],
      "metadata": {
        "id": "yLWCEo-gWTVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Categorias            | Descrições                                                                        |\n",
        "| --------------------- | --------------------------------------------------------------------------------- |\n",
        "| Assédio               | Comentários negativos ou nocivos voltados à identidade e/ou atributos protegidos. |\n",
        "| Discurso de ódio      | Conteúdo grosseiro, desrespeitoso ou linguagem obscena.                           |\n",
        "| Sexualmente explícito | Contém referências a atos sexuais ou outro conteúdo sexual.                       |\n",
        "| Material perigoso     | Promove, facilita ou incentiva atos nocivos.                                      |"
      ],
      "metadata": {
        "id": "jKU8dfBMWMCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Limite (API)                     | Descrição                                                                     |\n",
        "| -------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| BLOCK_NONE                       | Sempre mostrar, independentemente da probabilidade de conteúdo não seguro     |\n",
        "| BLOCK_ONLY_HIGH                  | Bloquear quando houver alta probabilidade de conteúdo não seguro              |\n",
        "| BLOCK_MEDIUM_AND_ABOVE           | Bloquear quando houver probabilidade média ou alta de conteúdo não seguro     |\n",
        "| BLOCK_LOW_AND_ABOVE              | Bloquear quando a probabilidade de conteúdo não seguro é baixa, média ou alta |\n",
        "| HARM_BLOCK_THRESHOLD_UNSPECIFIED | O limite não foi especificado, foi bloqueado usando o limite padrão           |"
      ],
      "metadata": {
        "id": "uuMw3CQdUzG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "safety_settings = {\n",
        "    \"HARASSMENT\":  \"BLOCK_MEDIUM_AND_ABOVE\" ,\n",
        "    \"HATE\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
        "    \"SEXUAL\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
        "    \"DANGEROUS\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
        "}"
      ],
      "metadata": {
        "id": "WuTFfrowsjxB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8vmtzAlPaQH-"
      },
      "outputs": [],
      "source": [
        "ordering_system = [add_to_order, get_order, remove_item, clear_order, confirm_order, place_order]\n",
        "\n",
        "# Toggle this to switch between Gemini 1.5 with a system instruction, or Gemini 1.0 Pro.\n",
        "use_sys_inst = False\n",
        "\n",
        "model_name = 'gemini-1.5-pro-latest' if use_sys_inst else 'gemini-1.0-pro-latest'\n",
        "\n",
        "if use_sys_inst:\n",
        "  model = genai.GenerativeModel(\n",
        "        model_name,\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        tools=ordering_system,\n",
        "        system_instruction=PIZZA_BOT_PROMPT\n",
        "      )\n",
        "  convo = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "else:\n",
        "  model = genai.GenerativeModel(\n",
        "        model_name,\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        tools=ordering_system)\n",
        "\n",
        "  convo = model.start_chat(\n",
        "      history=[\n",
        "          {'role': 'user', 'parts': [PIZZA_BOT_PROMPT, 'Responda apenas em portugues do brasil']},\n",
        "          {'role': 'model', 'parts': ['Sim eu entendi. Eu darei o meu melhor e irei interagir apenas em portugues do Brasil!']}\n",
        "        ],\n",
        "      enable_automatic_function_calling=True)\n",
        "\n",
        "\n",
        "@retry.Retry(initial=10)\n",
        "def send_message(message):\n",
        "  return convo.send_message(message)\n",
        "\n",
        "\n",
        "placed_order = []\n",
        "order = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624ai2Q5WMAF"
      },
      "source": [
        "## Faça seu pedido!\n",
        "\n",
        "Com o modelo definido e o bate-papo criado, só falta conectar a entrada do usuário ao modelo e exibir a saída, em loop. Esse loop continua até que um pedido completo seja feito.\n",
        "\n",
        "Quando executado no Colab, qualquer texto de largura fixa se origina do seu código Python (por exemplo, chamadas de print no sistema de pedidos), o texto regular vem da API do Gemini e as caixas delimitadas permitem a entrada do usuário, que é renderizada com um > inicial.\n",
        "\n",
        "E como já ouvi muitas vezes, **tudo acaba em pizza**!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "38SAyrNNVhvE",
        "outputId": "f84d34dd-c45a-437a-cb65-66a256bd1089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bem vindo a Pizzaria Botsarello! \n",
            "\n",
            "\n",
            "\n",
            "Horário de Funcionamento: Terças aos Domingos das 5pm às 2am\n",
            "Preços: Todas as pizzas tem preço fixo de R$ 60,00.\n",
            "\n",
            "MENU:\n",
            "Bordas para todas as pizzas:\n",
            "borda de catupiry\n",
            "borda de cheddar\n",
            "borda de parmesão\n",
            "borda de nutela\n",
            "\n",
            "\n",
            "Pizzas Classicas:\n",
            "Mussarela\n",
            "Napolitana\n",
            "Calabresa\n",
            "Marguerita\n",
            "\n",
            "Pizzas Sobremesa:\n",
            "Banana\n",
            "Chocolate\n",
            "Morango com Chocolate\n",
            "\n",
            "Pizzas Stranger Things:\n",
            "Jiló \n",
            "\n",
            "\n",
            "> boa tarde, uma pizza de jilo!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sinto muito, mas não temos mais pizza de Jiló hoje."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> então mussarela com borda de parmessão\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ok, uma pizza de mussarela com borda de parmesão. Mais alguma coisa?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> uma pizza napolitana com borda de catupiry\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ok, uma pizza de mussarela com borda de parmesão e uma pizza napolitana com borda de catupiry. Mais alguma coisa?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> uma de banana com nutela\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ok, uma pizza de mussarela com borda de parmesão, uma pizza napolitana com borda de catupiry e uma pizza de banana com borda de nutela. Mais alguma coisa?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> só isso, pode fechar\n",
            "Seu Pedido:\n",
            "  Mussarela\n",
            "   - borda de parmesão\n",
            "  Napolitana\n",
            "   - borda de catupiry\n",
            "  Banana\n",
            "   - borda de nutela\n",
            "O pedido está correto? Uma de cada certo?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Uma de cada certo?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> certo\n",
            "  Nota fical enviada para customer@mail.com\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "O seu pedido está confirmado e levará aproximadamente 6 minutos para ficar pronto."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "[Botsarello session over]\n",
            "\n",
            "Seu Pedido:\n",
            "  [('Mussarela', ['borda de parmesão']), ('Napolitana', ['borda de catupiry']), ('Banana', ['borda de nutela'])]\n",
            "\n",
            "- Muito obrigado pela jornada!!\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "print('Bem vindo a Pizzaria Botsarello! \\n')\n",
        "\n",
        "print(\"\"\"\n",
        "Horário de Funcionamento: Terças aos Domingos das 5pm às 2am\n",
        "Preços: Todas as pizzas tem preço fixo de R$ 60,00.\n",
        "\n",
        "MENU:\n",
        "Bordas para todas as pizzas:\n",
        "borda de catupiry\n",
        "borda de cheddar\n",
        "borda de parmesão\n",
        "borda de nutela\n",
        "\n",
        "\n",
        "Pizzas Classicas:\n",
        "Mussarela\n",
        "Napolitana\n",
        "Calabresa\n",
        "Marguerita\n",
        "\n",
        "Pizzas Sobremesa:\n",
        "Banana\n",
        "Chocolate\n",
        "Morango com Chocolate\n",
        "\n",
        "Pizzas Stranger Things:\n",
        "Jiló \\n\\n\"\"\")\n",
        "\n",
        "while not placed_order:\n",
        "  response = send_message(input('> '))\n",
        "  display(Markdown(response.text))\n",
        "\n",
        "\n",
        "print('\\n\\n')\n",
        "print('[Botsarello session over]')\n",
        "print()\n",
        "print('Seu Pedido:')\n",
        "print(f'  {placed_order}\\n')\n",
        "print('- Muito obrigado pela jornada!!')"
      ]
    }
  ]
}